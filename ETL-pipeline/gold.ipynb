{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50ba787c-18ad-4b67-8484-6a221b6f06b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "137e3079-fdb2-4492-80ad-bcaa0823bde0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Gold class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04612aa-ca95-473b-9c96-585cd9a009d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "\n",
    "class Gold():\n",
    "    def __init__(self, dataset_names, gold_catalog, silver_catalog, schema, volume_to_export=\"export\"):\n",
    "        '''Defining catalogs & schema.'''\n",
    "        self.dataset_names = dataset_names\n",
    "        self.gold_catalog = gold_catalog \n",
    "        self.silver_catalog = silver_catalog\n",
    "        self.schema = schema\n",
    "        self.common_cols = set()\n",
    "        self.volume_to_export = volume_to_export\n",
    "        self.export_path = f\"/Volumes/{self.gold_catalog}/{self.schema}/{self.volume_to_export}\"\n",
    "\n",
    "        # Creating the volume to which the .csv file is saved\n",
    "        spark.sql(f\"CREATE VOLUME IF NOT EXISTS {self.gold_catalog}.{self.schema}.{self.volume_to_export}\") \n",
    "\n",
    "    def read_silver_data(self, dataset_names):\n",
    "        datasets = []\n",
    "        for dataset_name in dataset_names:\n",
    "            # Read data\n",
    "            df = spark.read.table(f'{self.silver_catalog}.{self.schema}.{dataset_name}_{self.silver_catalog}')\n",
    "            datasets.append(df)\n",
    "            # Get common columns used in join\n",
    "            if len(self.common_cols) == 0:\n",
    "                self.common_cols.update(df.columns)\n",
    "            else:\n",
    "                self.common_cols = self.common_cols.intersection(df.columns)\n",
    "        return datasets\n",
    "\n",
    "    def execute_gold_pipeline(self):\n",
    "        print(\"Reading data...\")\n",
    "        datasets = self.read_silver_data(self.dataset_names)\n",
    "        common_cols = list(self.common_cols)\n",
    "\n",
    "        print(\"Joining tables...\")\n",
    "        gold_table = reduce(\n",
    "            lambda left, right: left.join(right, on=common_cols, how='outer'), datasets\n",
    "        )\n",
    "        \n",
    "        # Save gold table\n",
    "        print(\"Saving data...\")\n",
    "        gold_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{self.gold_catalog}.{self.schema}.gold_table\")\n",
    "\n",
    "        # Export .csv file\n",
    "        print(\"Creating .csv export...\")\n",
    "        (gold_table.coalesce(1)\n",
    "            .write.mode(\"overwrite\")\n",
    "            .option(\"header\", True)\n",
    "            .csv(f\"{self.export_path}/gold_table\"))\n",
    "        \n",
    "        print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f402c1b0-83ce-438f-9983-d37d28dbd9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Creating the Gold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc0851ba-fae7-4733-b167-de8028b8d6ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_names = ['visits', 'customers', 'visits_customers']\n",
    "\n",
    "gold = Gold(dataset_names, 'gold', 'silver', 'avohilmo', volume_to_export=\"export\")\n",
    "gold.execute_gold_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b686ba5-fab4-4913-bda7-c8006af4e11c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
