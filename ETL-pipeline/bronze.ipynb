{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d16a650-68da-4eb9-8f97-960fa205fa2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Extract/Ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7638f3c6-8ad8-45ae-8d70-d7c4e64db3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Checking Unity Catalog is enabled \n",
    "The command found from [here](https://docs.databricks.com/aws/en/data-governance/unity-catalog/get-started#run-a-sql-query-to-confirm-unity-catalog-enablement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdba28f4-e476-4e6c-a93f-82483c10058b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT CURRENT_METASTORE();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e909de9-c5a3-4028-a1ed-2466112d307d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Bronze class\n",
    "\n",
    "Methods used in extracting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc1d319-b638-4ef1-830c-74b4a8dbb5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import requests\n",
    "# (Py)Spark is intialized in Databricks Python notebooks automatically, thus below imports are redundant if the variable is not overwritten.\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "class Bronze():\n",
    "    def __init__(self, dataset_names, urls, catalog, schema):\n",
    "        '''Setting catalog, schema, creating volume, defining paths'''\n",
    "        self.dataset_names = dataset_names\n",
    "        self.urls = urls\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.volume = \"raw\"\n",
    "        self.landing_zone = f\"/Volumes/{self.catalog}/{self.schema}/{self.volume}\"\n",
    "        self.raw_data_paths = [f\"{self.landing_zone}/{data_name}.csv\" for data_name in self.dataset_names]\n",
    "        self.bronze_tables = [f\"{data_name}_{self.catalog}\" for data_name in dataset_names]\n",
    "\n",
    "        spark.sql(f\"USE {self.catalog}.{self.schema}\") # Setting the catalog & schema for the whole notebook\n",
    "        spark.sql(f\"CREATE VOLUME IF NOT EXISTS {self.volume}\") # Create volume for raw data landing zone\n",
    "\n",
    "    def get_response(self, URL):\n",
    "        '''Reads a URL and returns a requests.Response object'''\n",
    "        return requests.get(URL, headers={\"User-Agent\": \"Databricks\"})\n",
    "\n",
    "    def df_from_response(self, response, raw_landing_path):\n",
    "        '''Converts data from requests.Response().content to a PySpark dataframe'''\n",
    "        response.raise_for_status()\n",
    "        with open(raw_landing_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        return spark.read.option(\"delimiter\", \";\").csv(raw_landing_path,header=True)\n",
    "\n",
    "    def df_to_UC_table(self, df, table_path):\n",
    "        '''Writes a PySpark dataframe to a UC table'''\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_path)\n",
    "\n",
    "    def bronze_pipeline(self, URL, raw_landing_path, table_path):\n",
    "        '''Read data, write to spark dataframe and save as UC table.'''\n",
    "        print(\"Reading data from \" + URL + \"...\")\n",
    "        response = self.get_response(URL)\n",
    "        print(\"Creating a Spark dataframe...\")\n",
    "        df = self.df_from_response(response, raw_landing_path)\n",
    "        print(\"Saving data to UC table...\")\n",
    "        self.df_to_UC_table(df, table_path)\n",
    "        return df\n",
    "    \n",
    "    def execute_bronze_pipeline(self):\n",
    "        '''Executes the bronze pipeline for all datasets'''\n",
    "        for url, land_zone, table_name in zip(self.urls, self.raw_data_paths, self.bronze_tables):\n",
    "            print(\"Prosessing the dataset: \" + table_name[:-(len(self.catalog)+1)])\n",
    "            df = self.bronze_pipeline(url, land_zone, table_name)\n",
    "            display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c620b48-0745-4323-ada0-17dc2c9ad834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Requesting the data\n",
    "\n",
    "Defining the data sources; We use the API for open datasets of THL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf57891-0f55-45ed-a8f8-1845ae134ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Kaynnit\n",
    "visits_url = \"https://sampo.thl.fi/pivot/prod/fi/avo/perus01/fact_ahil_perus01.csv?row=ammatti-30664&column=aika-87596&column=palvelumuoto-33780#\"\n",
    "\n",
    "# Asiakkaat\n",
    "customers_url = \"https://sampo.thl.fi/pivot/prod/fi/avo/perus01/fact_ahil_perus01.csv?row=ammatti-30664&column=aika-87596&column=palvelumuoto-33780&filter=measure-87454#\"\n",
    "\n",
    "# Kaynnit/Asiakkaat\n",
    "visits_customers_url = \"https://sampo.thl.fi/pivot/prod/fi/avo/perus01/fact_ahil_perus01.csv?row=ammatti-30664&column=aika-87596&column=palvelumuoto-33780&filter=measure-87613#\"\n",
    "\n",
    "urls = [visits_url, customers_url, visits_customers_url]\n",
    "dataset_names = [\"visits\", \"customers\", \"visits_customers\"]\n",
    "\n",
    "bronze = Bronze(dataset_names, urls, 'bronze', 'avohilmo')\n",
    "bronze.execute_bronze_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c775020-84d9-4908-a114-78f392c382ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8902074385390792,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
